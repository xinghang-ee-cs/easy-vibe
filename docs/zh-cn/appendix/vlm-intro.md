# 多模态大模型入门 (VLM Intro)

> 💡 **学习指南**：本章节无需深厚的计算机视觉背景，通过交互式演示带你理解 AI 是如何拥有“眼睛”的。我们将揭秘 GPT-4V、Qwen-VL 等模型背后的核心原理。

<VlmQuickStartDemo />

## 0. 引言：给大脑装上眼睛

在 [大语言模型入门](./llm-intro) 中，我们知道 LLM 本质上是一个被关在黑盒子里、只能通过**文字**来了解世界的“大脑”。

**多模态大模型 (VLM)** 的出现，相当于给这个大脑装上了一双**眼睛**。

但这并不容易。因为：
- **大脑 (LLM)** 只懂**文字**（准确说是 Token ID）。
- **眼睛 (摄像头)** 看到的是**像素**（RGB 颜色数值）。

VLM 的核心任务，就是**把“像素信号”翻译成“文字信号”**，让 LLM 觉得看图就像读文章一样简单。

---

## 1. 第一步：把图片变成“单词” (Visual Tokenization)

想象一下，你正在电话里给朋友描述一副拼图。你不可能一口气说完，你得一块一块地描述。
计算机看图也是一样的道理。

### 1.1 切块 (Patchify) —— 制作视觉单词

LLM 习惯读单词。为了配合它，我们得把一张完整的图片切成一个个小方块（Patch）。

- **图片** = 一篇文章
- **方块 (Patch)** = 一个单词

通常，我们会把图片切成 $16 \times 16$ 像素的小方块。一张 $224 \times 224$ 的图片就会变成 $14 \times 14 = 196$ 个方块。

> 🕹️ **交互演示**：点击下方按钮，看图片是如何被“切”成单词的。

<PatchifyDemo />

### 1.2 序列化 (Flatten) —— 排成一句话

切完后，我们得到的是一个 $14 \times 14$ 的方阵。但 LLM 只能读**一行字**（序列）。
所以，我们必须把这个方阵**拍扁**，变成长长的一串。

- **原本**：二维矩阵（有行有列）。
- **现在**：一维长条（只有前后）。

这样，图片就变成了一串“视觉单词序列”。

下面的演示展示了：**一个 Patch** 是如何被拍扁，并变成一个**向量**（计算机能读懂的数字列表）的。

<LinearProjectionDemo />

---

## 2. 第二步：跨物种翻译 (Projection)

现在我们有了一串“视觉单词”，但 LLM 还是读不懂。
因为这些“视觉单词”是**像素特征**（比如“这里是红色”、“那里有条线”），而 LLM 懂的是**语义特征**（比如“这是猫”、“那是树”）。

这就需要一个翻译官：**Projector (投射器)**。

### 2.1 翻译官的作用

Projector 的工作就是把**视觉特征向量**（ViT 的输出）转换成**文本特征向量**（LLM 的输入）。

你可以把它理解为**外语翻译器**：
- **输入**：视觉语言（ViT output）
- **处理**：翻译（矩阵变换）
- **输出**：LLM 语言（LLM embedding）

<ProjectorDemo />

### 2.2 不同的翻译流派

为了翻译得更好，科学家们发明了不同的翻译工具：

1.  **直译派 (Linear)**：
    - 做法：简单粗暴，通过一个矩阵乘法直接转换。
    - 特点：**保留原汁原味**，但废话多（Token 数量多）。
    - 代表：LLaVA。

2.  **意译派 (Q-Former/Resampler)**：
    - 做法：用一个小模型先读一遍图片，总结出几十个核心要点。
    - 特点：**精简**，Token 少，但可能会漏掉细节。
    - 代表：BLIP-2, Gemini。

3.  **折中派 (C-Abstractor)**：
    - 做法：把相邻的几个方块合并成一个，既压缩了长度，又保留了空间感。
    - 代表：Qwen-VL。

---

## 3. 第三步：合体 (The Architecture)

现在，零件都准备好了，我们把它们组装起来，就成了一个标准的 VLM。

### 3.1 VLM 的身体结构

<ModelArchitectureComparisonDemo />

一个典型的 VLM（如 LLaVA）由三个部分组成：

1.  **眼睛 (Vision Encoder)**：
    - 负责看图。
    - 通常直接借用现成的、训练好的视觉模型（如 CLIP, SigLIP）。
    - *它就像视网膜，负责感光。*

2.  **视神经 (Projector)**：
    - 负责传输和翻译信号。
    - 这是 VLM 训练的重点。
    - *它连接眼睛和大脑。*

3.  **大脑 (LLM)**：
    - 负责思考和回答。
    - 借用现成的强大 LLM（如 Vicuna, Qwen）。
    - *它负责理解看到了什么，并组织语言回答。*

---

## 4. 它是怎么学会看图的？(Training)

刚组装好的 VLM 其实是“瞎”的，因为视神经（Projector）还没连通。我们需要分两步教它。

### 阶段一：认物 (Feature Alignment)

这一阶段就像教婴儿认卡片。

- **给它看**：一张“猫”的照片。
- **告诉它**：这是“猫”。
- **目标**：让 Projector 学会把“猫的照片特征”翻译成“猫这个字的向量”。
- **状态**：冻结眼睛和大脑，**只训练视神经 (Projector)**。

<FeatureAlignmentDemo />

### 阶段二：对话 (Visual Instruction Tuning)

这一阶段是教它根据图片回答复杂问题。

- **用户问**：`<图片>` 图里的猫在干什么？
- **教它答**：它在睡觉。
- **目标**：让大脑 (LLM) 学会处理视觉信息，并结合常识进行推理。
- **状态**：通常会同时微调 **Projector** 和 **LLM**。

<VLMInferenceDemo />

---

## 5. 进阶：看得更清 (Advanced Tricks)

基础的 VLM 有个大问题：**视力不好**。
传统的 ViT 只能看 $224 \times 224$ 或 $336 \times 336$ 分辨率的图。这就像透过一个低清摄像头看世界，小字根本看不清。

现在的模型（如 Qwen-VL, LLaVA-NeXT）用了一些聪明的方法来解决这个问题：

### 5.1 动态分辨率 (Dynamic Resolution)

简单说，就是**“拼图法”**。

如果图片很大（比如 $1000 \times 1000$），模型不会强行把它缩小，而是：
1.  把它切成好几张 $336 \times 336$ 的小图。
2.  分别看这些小图（看细节）。
3.  再把全图缩小看一遍（看全貌）。
4.  最后把所有信息拼起来。

这就好比你用手机拍全景照片，分段扫描，最后合成一张高清大图。

### 5.2 换个大眼睛

还有一种暴力美学：直接换一个更强的视觉模型。
比如 **InternVL**，直接用了一个 60 亿参数的超大视觉模型（InternViT-6B）。
这相当于从“手机摄像头”升级到了“哈勃望远镜”，不用切图也能看得一清二楚。

---

## 6. 总结

多模态大模型 (VLM) 并没有什么魔法。它只是做了一件事：

**把“图像”这种外语，翻译成了“文本”这种母语，然后喂给了 LLM。**

只要理解了这一点，你就理解了 VLM 的一切。

---

## 7. 名词速查表 (Glossary)

| 名词 | 全称 | 解释 |
| :--- | :--- | :--- |
| **VLM** | Vision-Language Model | **多模态大模型**。能看懂图的 GPT。 |
| **ViT** | Vision Transformer | **视觉模型**。VLM 的“眼睛”，负责把像素变成向量。 |
| **Patch** | - | **图像块**。图片被切成的小方块，相当于“视觉单词”。 |
| **Projector** | - | **投射器/翻译官**。连接眼睛和大脑的桥梁。 |
| **Alignment** | - | **对齐**。让图像特征和文本特征在同一个空间里“互相听得懂”。 |
