# 上下文工程入门 (Context Engineering)

> 💡 **学习指南**：如果说 Prompt Engineering 是教 AI "怎么说话"，那么 Context Engineering 就是教 AI "怎么记事"。本章节将通过一系列交互式实验，带你深入理解 AI 的记忆机制，从基础的滑动窗口到高级的 RAG 系统，掌握让 AI "过目不忘"的核心技术。

在开始之前，建议你先了解两个概念：

- **Token 是什么**：可以先阅读 [大语言模型入门](./llm-intro.md) 的「分词 & Token」部分。
- **Prompt 是什么**：如果你还不熟悉 System / User / Assistant 的基本结构，可以先看 [提示词工程](./prompt-engineering/)。

<AgentContextFlow />

## 0. 引言：金鱼与大象

想象一下，你正在和两个人聊天：

- **A (金鱼记忆)**：只能记住最后说的 3 句话。如果你问他"我刚才说了什么？"，他可能会一脸茫然。
- **B (大象记忆)**：能记住你们聊过的每一句话，甚至是你上个月提到的细节。

**上下文工程 (Context Engineering)** 的目标，就是通过技术手段，让 AI 从 "金鱼" 进化成 "大象"。

更具体地说：你每次调用模型时，都会把一份「输入包」发给它，这份输入包通常由这些部分拼起来：

- **系统设定**（System Prompt）：角色、规则、边界。
- **对话历史**（Messages）：你们之前聊过什么。
- **工具结果**（Tool / Observation）：Agent 调用外部工具拿到的新信息。
- **检索片段**（RAG Context）：从知识库临时取回的相关内容。

但这里有一个核心挑战：**AI 的"脑容量"（上下文窗口）是有限的**。我们不能把全世界的信息都塞进去。

我们需要解决五个核心问题：

1.  **容量限制**：到底能装多少东西？
2.  **遗忘机制**：装满了怎么办？
3.  **智能保留**：如何只忘掉不重要的？
4.  **长期记忆**：怎么记住很久以前的事？
5.  **信息压缩**：怎么把书读薄？

---

## 1. 第一步：理解瓶颈 (The Context Window)

### 1.1 什么是上下文窗口？

大语言模型 (LLM) 的记忆不是无限的。它有一个固定的**上下文窗口 (Context Window)**，就像一个只能写 1000 个字的黑板。一旦写满，要么擦掉旧的，要么停止写入。

### 1.2 实验：Token 与容量

在 AI 的世界里，计量的单位不是"字"，而是 **Token**。（Token 的更完整解释可以回看 [大语言模型入门](./llm-intro.md)。）
粗略地说，一个 Token 大约相当于 0.75 个英文单词，或 0.5-1 个汉字（会因内容而变化）。

试着在下面的模拟器中输入文字，看看它是如何填满上下文窗口的：

<ContextWindowVisualizer />

**关键点**：

- **溢出即丢失**：一旦超过红色警戒线，模型可能会报错，或者直接截断后面的内容。
- **昂贵的记忆**：上下文越长，推理速度越慢，费用也越高。

### 1.3 额外收益：前缀稳定与缓存命中

在 Agent 场景里，上下文通常是「系统设定 + 工具定义 + 历史消息 + 本轮新信息」的拼接。
如果你能让这份输入包的**前半段尽量稳定**（比如系统提示、工具列表不要频繁变动），很多模型/服务会更容易复用缓存，从而降低延迟与成本。

---

## 2. 第二步：即时记忆 (Sliding Window)

### 2.1 问题：聊久了就忘

当对话持续进行，Token 数量不断增加，最终会通过窗口限制。最简单的处理方式是**滑动窗口 (Sliding Window)**。

### 2.2 解决方案：先进先出 (FIFO)

就像一个滑动的相框，当新消息进来时，最旧的消息被"挤"出画面，被彻底遗忘。

<SlidingWindowDemo />

**观察**：

- 当对话填满窗口后，最早的 `User` 消息变灰并消失。
- **缺陷**：如果我在第一句话里告诉 AI "我的名字叫小明"，几轮对话后，它就忘了我叫什么。

---

## 3. 第三步：智能管理 (Selective Retention)

### 3.1 问题：重要的事不能忘

"滑动窗口"太笨了，它平等地对待每一句话。但有些信息（如你的名字、任务目标、系统设定）是**全局重要**的，无论对话多长都不能忘。

### 3.2 解决方案：选择性保留 (Smart Context)

我们需要一种机制，将关键信息**钉 (Pin)** 在窗口里，不受滑动影响。这通常通过 `System Prompt` 或动态注入来实现。

<SelectiveContextDemo />

**实验指南**：

1.  观察顶部的 **📌 Pinned** 区域，这里的信息永远不会被挤走。
2.  在下方添加新消息，注意观察 **📜 Scrolling** 区域的变化。
3.  尝试点击某条消息旁边的 📌 按钮，把它变成"永久记忆"。

**原理**：我们牺牲了一部分流动窗口的空间，换取了关键信息的持久性。

---

## 4. 第四步：长期记忆 (RAG & Vector DB)

### 4.1 问题：如何记住一本书？

如果我们需要 AI 记住几百页的技术文档，或者你过去一年的日记，即使是 "Pinned" 也装不下（窗口太贵且有限）。

### 4.2 解决方案：外挂大脑 (RAG)

**检索增强生成 (RAG, Retrieval-Augmented Generation)** 是目前的终极解决方案。我们不把所有记忆都塞进大脑，而是把它们写在"笔记本"（向量数据库）里。

当需要回答问题时，先去笔记本里**检索**相关的那一页，临时读入大脑。

<RAGSimulationDemo />

**流程解析**：

1.  **Embedding**：将你的问题变成数学向量。
2.  **Search**：在数据库中寻找"长得像"（语义相似）的片段。
3.  **Retrieve**：只把那 2-3 个相关片段取出来。
4.  **Augment**：把这些片段和你的问题一起塞进 Prompt。

这样，AI 就能回答它从未"背过"的知识了。

---

## 5. 第五步：信息压缩 (Compression)

### 5.1 问题：检索出来的内容还是太长

有时候，即使检索出的相关片段也太长了。我们需要一种方法，在保留核心含义的前提下，减少 Token 消耗。

### 5.2 解决方案：上下文压缩

我们可以用更小的模型，或者专门的算法，对文本进行压缩。

<ContextCompressionDemo />

**常见策略**：

- **Summarize**：用自然语言总结大意。适合理解整体脉络。
- **Extract Key Points**：提取要点列表。适合逻辑性强的内容。
- **JSON Structure**：提取结构化数据。适合程序处理。

---

## 6. 总结：构建 AI 的记忆宫殿

上下文工程不仅仅是简单的"拼接字符串"，它是一个精密的系统工程：

| 阶段         | 核心技术     | 适用场景           | 类比                |
| :----------- | :----------- | :----------------- | :------------------ |
| **L1. 限制** | Token 计数   | 了解边界           | 黑板的大小          |
| **L2. 短期** | 滑动窗口     | 日常闲聊           | 只能记 3 句话的金鱼 |
| **L3. 关键** | 选择性保留   | 角色扮演、任务设定 | 手心写字的备忘录    |
| **L4. 长期** | RAG / 向量库 | 知识库问答         | 随时查阅的图书馆    |
| **L5. 优化** | 压缩 / 摘要  | 降低成本、提速     | 读书笔记            |

掌握了这些，你就掌握了控制 AI "注意力"的钥匙。现在，去构建属于你的长记忆 AI 应用吧！
